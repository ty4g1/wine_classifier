{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Classifier (Using Pseudo labeling)\n",
    "\n",
    "Now that we've seen how unsupervised learning and supervised learning can be used to classify on the wines dataset. Lets try something different. We will combine these two techniques. We will assume our data to be unlabeled, and use unsupervised learning to generate labels for the data. Now that we have our \"labeled\" data, we will train a supervised learning model on this data.\n",
    "\n",
    "After this, we can compare the performance of unsupervised learning, supervised learning, and supervised learning with pseudo-labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did for purely unsupervised learning, we will first drop the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "wine = load_wine(as_frame=True)\n",
    "wine_df = wine.frame\n",
    "wine_df.head()\n",
    "\n",
    "# Drop target column\n",
    "wine_df_unlabeled = wine_df.drop(['target'], axis=1)\n",
    "wine_df_unlabeled.head()\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "scaled_wine_df_unlabeled = scaler.fit_transform(wine_df_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already know the number of optimal clusters to be 3 from our earlier attempt, we will skip the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centroids\n",
      "[[-0.92607185 -0.39404154 -0.49451676  0.17060184 -0.49171185 -0.07598265\n",
      "   0.02081257 -0.03353357  0.0582655  -0.90191402  0.46180361  0.27076419\n",
      "  -0.75384618]\n",
      " [ 0.16490746  0.87154706  0.18689833  0.52436746 -0.07547277 -0.97933029\n",
      "  -1.21524764  0.72606354 -0.77970639  0.94153874 -1.16478865 -1.29241163\n",
      "  -0.40708796]\n",
      " [ 0.83523208 -0.30380968  0.36470604 -0.61019129  0.5775868   0.88523736\n",
      "   0.97781956 -0.56208965  0.58028658  0.17106348  0.47398365  0.77924711\n",
      "   1.12518529]]\n",
      "\n",
      "Sample labels\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2004e\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = scaled_wine_df_unlabeled\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "print(\"Cluster centroids\")\n",
    "print(kmeans.cluster_centers_)\n",
    "print()\n",
    "print(\"Sample labels\")\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our knowledge of the data set, we can map the cluster labels to the corresponding target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Assigning clusters to wine targets\n",
    "label_to_class_map = {0: 1, 1: 2, 2: 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create our own target column based on the labels we have obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_labeled_wine_df = wine_df_unlabeled.copy()\n",
    "pseudo_labeled_wine_df['target'] = pd.Series(kmeans.labels_).map(lambda l: label_to_class_map[l])\n",
    "pseudo_labeled_wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our \"labeled\" data, we can perform supervised learning using neural networks on it. The method will be the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pseudo_labeled_wine_df.loc[:, :'proline'].values\n",
    "y = pseudo_labeled_wine_df.target.values\n",
    "y_true = wine.target.values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# We want to test against the true labels\n",
    "\n",
    "_, y_test = train_test_split(y_true, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to pytorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# Define model class\n",
    "class WineClassifier(nn.Module):\n",
    "    def __init__(self, input_size=13, hidden_size=8, num_classes=3):\n",
    "        super(WineClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "model = WineClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let us now train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6007\n",
      "Train Accuracy: 0.9437, Test Accuracy: 0.9167\n",
      "Epoch [20/100], Loss: 0.2630\n",
      "Train Accuracy: 0.9718, Test Accuracy: 0.9444\n",
      "Epoch [30/100], Loss: 0.1199\n",
      "Train Accuracy: 0.9930, Test Accuracy: 1.0000\n",
      "Epoch [40/100], Loss: 0.0664\n",
      "Train Accuracy: 0.9930, Test Accuracy: 1.0000\n",
      "Epoch [50/100], Loss: 0.0439\n",
      "Train Accuracy: 0.9930, Test Accuracy: 1.0000\n",
      "Epoch [60/100], Loss: 0.0330\n",
      "Train Accuracy: 0.9930, Test Accuracy: 1.0000\n",
      "Epoch [70/100], Loss: 0.0268\n",
      "Train Accuracy: 0.9930, Test Accuracy: 1.0000\n",
      "Epoch [80/100], Loss: 0.0226\n",
      "Train Accuracy: 0.9930, Test Accuracy: 1.0000\n",
      "Epoch [90/100], Loss: 0.0196\n",
      "Train Accuracy: 0.9930, Test Accuracy: 1.0000\n",
      "Epoch [100/100], Loss: 0.0172\n",
      "Train Accuracy: 0.9930, Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Define training loop\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()                           # Kind of put the model into training mode\n",
    "    optimizer.zero_grad()                   # Reset gradient values for parameters\n",
    "    output = model(X_train)                 # Do a forward pass\n",
    "    loss = criterion(output, y_train)       # Compute loss\n",
    "    loss.backward()                         # Do a backward pass\n",
    "    optimizer.step()                        # Update weights\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    _, y_pred = torch.max(output, 1)\n",
    "    train_accuracy = (y_pred == y_train).sum().item() / y_train.size(0)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "        test_accuracy = (test_predicted == y_test).sum().item() / y_test.size(0)\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(loss.item())\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        print(f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets get some metrics to compare our new model based on pseudo-labeled data, to our previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy: 0.6111\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_test_outputs = model(X_test)\n",
    "    _, final_predicted = torch.max(final_test_outputs.data, 1)\n",
    "    final_accuracy = (final_predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f'\\nFinal Test Accuracy: {final_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained results that suggest our new model accuracy is better than random classification (0.33 for 3 classes), but worse than a purely supervised learning approach. There are also a few drawbacks when it comes to generalizing this approach. Our current dataset was quite small, and thus we cannot say the same will work for much larger data sets. Additionally, we had a low number of features, and our data happened to be separated well enough for clustering to work. \n",
    "\n",
    "The main potential I see for this approach is when working with data that does not have \"true\" labels. When the labels are subjective, such as emotion, mood etc. These can be derived from natural groupings of the data, and a model can be trained on these labels to predict the same for unseen data. In such cases, the decision boundaries are expected to be fuzzy, and imperfections can be forgiven more easily."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
